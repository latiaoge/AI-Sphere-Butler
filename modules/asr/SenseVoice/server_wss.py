from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException, UploadFile, File
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
from pydantic_settings import BaseSettings
from pydantic import BaseModel, Field
from funasr import AutoModel
import numpy as np
import soundfile as sf
import argparse
import uvicorn
from urllib.parse import parse_qs
import os
import re
import glob
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from loguru import logger
import sys
import json
import traceback
import time
from pydub import AudioSegment
import io
import pypinyin
from pypinyin import Style
import webrtcvad
import asyncio

logger.remove()
log_format = "{time:YYYY-MM-DD HH:mm:ss} [{level}] {file}:{line} - {message}"
logger.add(sys.stdout, format=log_format, level="DEBUG", filter=lambda record: record["level"].no < 40)
logger.add(sys.stderr, format=log_format, level="ERROR", filter=lambda record: record["level"].no >= 40)


REGISTERED_SPEAKERS_FILE = "registered_speakers.pkl"

def load_registered_speakers():
    """ä»æ–‡ä»¶åŠ è½½å·²æ³¨å†Œç”¨æˆ·"""
    if os.path.exists(REGISTERED_SPEAKERS_FILE):
        try:
            with open(REGISTERED_SPEAKERS_FILE, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æ³¨å†Œç”¨æˆ·å¤±è´¥: {e}")
    return {}

def save_registered_speakers(speakers):
    """ä¿å­˜å·²æ³¨å†Œç”¨æˆ·åˆ°æ–‡ä»¶"""
    try:
        with open(REGISTERED_SPEAKERS_FILE, 'wb') as f:
            pickle.dump(speakers, f)
        logger.info(f"å·²ä¿å­˜ {len(speakers)} ä¸ªæ³¨å†Œç”¨æˆ·")
    except Exception as e:
        logger.error(f"ä¿å­˜æ³¨å†Œç”¨æˆ·å¤±è´¥: {e}")

def reg_spk_init(files):
    reg_spk = {}
    for f in files:
        try:
            data, sr = sf.read(f, dtype="float32")
            k, _ = os.path.splitext(os.path.basename(f))
            reg_spk[k] = {"data": data, "sr": sr}
            logger.info(f"æˆåŠŸåŠ è½½å£°çº¹æ–‡ä»¶: {k} from {f}")
        except Exception as e:
            logger.error(f"åŠ è½½å£°çº¹æ³¨å†Œæ–‡ä»¶{f}å¤±è´¥: {e}")
    return reg_spk


def get_latest_speaker_files(speaker_dir="speaker", max_files=1):
    if not os.path.exists(speaker_dir):
        os.makedirs(speaker_dir, exist_ok=True)
        return []
    files = glob.glob(os.path.join(speaker_dir, "*.wav"))
    files.sort(key=os.path.getmtime, reverse=True)
    return files[:max_files]


reg_spks_files = get_latest_speaker_files("speaker", max_files=1)
if not reg_spks_files:
    
    reg_spks_files = [
        "speaker/speaker1_a_cn_16k.wav"
    ]
reg_spks = reg_spk_init(reg_spks_files)


try:
    import pickle
    registered_speakers = load_registered_speakers()
    logger.info(f"å·²åŠ è½½ {len(registered_speakers)} ä¸ªæ³¨å†Œç”¨æˆ·")
    for spk in registered_speakers:
        logger.info(f"æ³¨å†Œç”¨æˆ·: {spk}")
except Exception as e:
    logger.error(f"åŠ è½½æ³¨å†Œç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
    registered_speakers = {}


class Config(BaseSettings):
    sv_thr: float = Field(0.40, description="Speaker verification threshold")  
    chunk_size_ms: int = Field(300, description="Chunk size in milliseconds")
    sample_rate: int = Field(16000, description="Sample rate in Hz")
    bit_depth: int = Field(16, description="Bit depth")
    channels: int = Field(1, description="Number of audio channels")
    avg_logprob_thr: float = Field(-0.25, description="average logprob threshold")
    min_speaker_verify_len: int = Field(12000, description="Minimum length for speaker verification (1.2 seconds)")
    
    sv_stability_threshold: float = Field(0.45, description="Speaker verification stability threshold")
    sv_failure_threshold: int = Field(3, description="Number of consecutive failures before resetting speaker identity")


config = Config()

emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
}

event_dict = {
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ¤§",
}

emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“",
    "<|zh|>": "",
    "<|en|>": "",
    "<|yue|>": "",
    "<|ja|>": "",
    "<|ko|>": "",
    "<|nospeech|>": "",
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
    "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
    "<|Sing|>": "",
    "<|Speech_Noise|>": "",
    "<|withitn|>": "",
    "<|woitn|>": "",
    "<|GBG|>": "",
    "<|Event_UNK|>": "",
}

lang_dict = {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·",}


def format_str(s):
    for sptk in emoji_dict:
        s = s.replace(sptk, emoji_dict[sptk])
    return s


def format_str_v2(s):
    sptk_dict = {}
    for sptk in emoji_dict:
        sptk_dict[sptk] = s.count(sptk)
        s = s.replace(sptk, "")
    emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    for e in event_dict:
        if sptk_dict[e] > 0:
            s = event_dict[e] + s
    s = s + emo_dict[emo]

    for emoji in emo_set.union(event_set):
        s = s.replace(" " + emoji, emoji)
        s = s.replace(emoji + " ", emoji)
    return s.strip()


def format_str_v3(s):
    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None

    def get_event(s):
        return s[0] if s[0] in event_set else None

    s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
    for lang in lang_dict:
        s = s.replace(lang, "<|lang|>")
    s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
    new_s = " " + s_list[0]
    cur_ent_event = get_event(new_s)
    for i in range(1, len(s_list)):
        if len(s_list[i]) == 0:
            continue
        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) is not None:
            s_list[i] = s_list[i][1:]
        cur_ent_event = get_event(s_list[i])
        if get_emo(s_list[i]) is not None and get_emo(s_list[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += s_list[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()


def contains_chinese_english_number(s: str) -> bool:
    return bool(re.search(r'[\u4e00-\u9fffA-Za-z0-9]', s))


sv_pipeline = pipeline(
    task='speaker-verification',
    model='iic/speech_eres2net_large_sv_zh-cn_3dspeaker_16k',
    model_revision='v1.0.0'
)

asr_pipeline = pipeline(
    task=Tasks.auto_speech_recognition,
    model='iic/SenseVoiceSmall',
    model_revision="master",
    device="cuda:0",
    disable_update=True
)

model_asr = AutoModel(
    model="iic/SenseVoiceSmall",
    trust_remote_code=True,
    remote_code="./model.py",
    device="cuda:0",
    disable_update=True
)

model_vad = AutoModel(
    model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
    model_revision="v2.0.4",
    disable_pbar=True,
    max_end_silence_time=500,
    disable_update=True
)


def reg_spk_init(files):
    reg_spk = {}
    for f in files:
        try:
            data, sr = sf.read(f, dtype="float32")
            k, _ = os.path.splitext(os.path.basename(f))
            reg_spk[k] = {
                "data": data,
                "sr": sr,
            }
        except Exception as e:
            logger.error(f"åŠ è½½å£°çº¹æ³¨å†Œæ–‡ä»¶{f}å¤±è´¥: {e}")
    return reg_spk



def speaker_verify(audio, sv_thr):
    """
    è¿”å›æ ¼å¼: (hit: bool, speaker_id: str, score: float, registered: bool)
    """
    hit = False
    best_score = 0.0
    best_speaker = None
    registered = False
    
    for speaker_id, speaker_data in reg_spks.items():
        res_sv = sv_pipeline([audio, speaker_data["data"]], sv_thr)
        current_score = res_sv["score"]
        
        logger.debug(
            f"ä¸è¯´è¯äºº {speaker_id} æ¯”å¯¹åˆ†æ•°: {current_score:.4f} (é˜ˆå€¼: {sv_thr})"
        )
        
        if current_score >= sv_thr and current_score > best_score:
            hit = True
            best_score = current_score
            best_speaker = speaker_id
            registered = True  
    
    logger.info(
        f"æœ€ç»ˆéªŒè¯ç»“æœ: hit={hit} speaker={best_speaker} "
        f"score={best_score:.4f} registered={registered}"
    )
    
    return hit, best_speaker, best_score, registered

def asr(audio, lang, cache, use_itn=False):
    start_time = time.time()
    result = model_asr.generate(
        input=audio,
        cache=cache,
        language=lang.strip(),
        use_itn=use_itn,
        batch_size_s=60,
    )
    end_time = time.time()
    elapsed_time = end_time - start_time
    logger.debug(f"asr elapsed: {elapsed_time * 1000:.2f} milliseconds")
    return result


def detect_wakeword(audio_chunk, cache_asr, wake_words):
    
    result = asr(audio_chunk, "zh", cache_asr, True)
    if result and len(result) > 0 and "text" in result[0]:
        text = result[0]["text"].lower()
        for wake_word in wake_words:
            if wake_word.lower() in text:
                logger.info(f"æ£€æµ‹åˆ°å”¤é†’è¯'{wake_word}'ï¼Œè¯†åˆ«æ–‡æœ¬ï¼š{text}")
                return True, text
    return False, ""


WAKE_WORDS = ["xiao li", "ni hao xiao li"]


class TranscriptionResponse(BaseModel):
    code: int
    info: str
    data: str


app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.websocket("/ws/transcribe")
async def websocket_endpoint(websocket: WebSocket):
    try:
        
        query_params = parse_qs(websocket.scope['query_string'].decode())
        sv = query_params.get('sv', ['false'])[0].lower() in ['true', '1', 't', 'y', 'yes']
        lang = query_params.get('lang', ['auto'])[0].lower()
        wakeword_enabled = query_params.get('wakeword', ['0'])[0].lower() in ['1', 'true', 'yes', 'on']
        wake_words = [w.strip().lower() for w in query_params.get('wakewords', [''])[0].split(',') if w.strip()] or WAKE_WORDS
        
        
        exit_commands = ["é€€ä¸‹å§", "ä¼‘æ¯å§", "å†è§", "é€€å‡º", "ç»“æŸ"]
        
        await websocket.accept()
        chunk_size = int(config.chunk_size_ms * config.sample_rate / 1000)
        
        
        audio_buffer_raw = b""  
        audio_buffer_processed = np.array([], dtype=np.float32)  
        audio_vad = np.array([], dtype=np.float32)  

        
        wakeword_buffer = np.array([], dtype=np.float32)  
        min_wake_samples = max(2400, int(config.sample_rate * 0.5))  
        sliding_window_samples = int(config.sample_rate * 0.2)  
        
        
        cache = {}  
        cache_asr = {}  
        cache_wake = {}  

        
        last_vad_beg = last_vad_end = -1  
        offset = 0  
        hit = False  
        flag_wakeword = not wakeword_enabled  
        current_speaker = None  
        
        
        min_asr_samples = 1600   # 100ms @16kHz
        max_asr_samples = int(config.sample_rate * 5)  
        wake_timeout = 300  
        
        
        consecutive_sv_failures = 0  
        sv_reset_threshold = config.sv_failure_threshold  
        
        
        wakeword_detected_time = 0  
        post_wake_audio = np.array([], dtype=np.float32)  
        
        
        asr_processing = False  
        
        
        last_activity_time = time.time()
        
        
        sv_buffer = np.array([], dtype=np.float32)
        
        
        last_asr_time = 0
        min_asr_interval = 0.5  
        
        
        is_speaking = False
        silent_frames = 0
        min_silent_frames = int(config.sample_rate * 0.5 / chunk_size)  
        
        logger.info(f"è¿æ¥å·²å»ºç«‹ | å‚æ•°: sv={sv}, lang={lang}, wakeword={wakeword_enabled}, wake_words={wake_words}")

        while True:
            
            if flag_wakeword and (time.time() - last_activity_time > wake_timeout):
                logger.info(f"å”¤é†’çŠ¶æ€è¶…æ—¶({wake_timeout}ç§’)ï¼Œè‡ªåŠ¨é€€å‡ºå”¤é†’çŠ¶æ€")
                flag_wakeword = False
                post_wake_audio = np.array([], dtype=np.float32)
                sv_buffer = np.array([], dtype=np.float32)
                await websocket.send_json({
                    "code": 4,
                    "info": "å”¤é†’çŠ¶æ€è¶…æ—¶ï¼Œå·²é€€å‡º",
                    "data": {}
                })
            
            
            try:
                data = await asyncio.wait_for(websocket.receive_bytes(), timeout=1.0)
                audio_buffer_raw += data
            except asyncio.TimeoutError:
                
                continue
            
            
            if len(audio_buffer_raw) >= 2:
                new_audio = np.frombuffer(
                    audio_buffer_raw[:len(audio_buffer_raw) - (len(audio_buffer_raw) % 2)], 
                    dtype=np.int16
                ).astype(np.float32) / 32767.0
                
                audio_buffer_processed = np.append(audio_buffer_processed, new_audio)
                audio_buffer_raw = audio_buffer_raw[len(audio_buffer_raw) - (len(audio_buffer_raw) % 2):]
                
                
                if flag_wakeword:
                    post_wake_audio = np.append(post_wake_audio, new_audio)
                    logger.debug(f"å”¤é†’è¯å·²æ¿€æ´»ï¼Œæ­£åœ¨æ”¶é›†éŸ³é¢‘: {len(post_wake_audio)} samples")
                    
                    
                    if sv:
                        sv_buffer = np.append(sv_buffer, new_audio)
                        
                        
                        if len(sv_buffer) >= config.min_speaker_verify_len:
                            try:
                                hit, speaker, score, registered = speaker_verify(sv_buffer, config.sv_thr)
                                
                                if hit:
                                    current_speaker = speaker
                                    consecutive_sv_failures = 0  
                                    logger.info(f"å£°çº¹éªŒè¯é€šè¿‡: {speaker} (åˆ†æ•°: {score:.4f})")
                                    await websocket.send_json({
                                        "code": 2,
                                        "info": "å£°çº¹éªŒè¯é€šè¿‡",
                                        "data": {
                                            "speaker": speaker,
                                            "score": float(score),
                                            "registered": registered
                                        }
                                    })
                                else:
                                    consecutive_sv_failures += 1
                                    logger.warning(f"å£°çº¹éªŒè¯å¤±è´¥: {consecutive_sv_failures}/{sv_reset_threshold}")
                                    
                                    
                                    if consecutive_sv_failures >= sv_reset_threshold:
                                        current_speaker = None
                                        consecutive_sv_failures = 0
                                        logger.info("å·²é‡ç½®å£°çº¹éªŒè¯çŠ¶æ€")
                                        await websocket.send_json({
                                            "code": 3,
                                            "info": "å£°çº¹éªŒè¯å·²é‡ç½®",
                                            "data": {}
                                        })
                                
                                
                                sv_buffer = np.array([], dtype=np.float32)
                            
                            except Exception as e:
                                logger.error(f"å£°çº¹éªŒè¯å¼‚å¸¸: {str(e)}\n{traceback.format_exc()}")
                                hit = False
                    
                    
                    if len(post_wake_audio) >= min_asr_samples:
                        
                        current_activity = check_vad_activity(post_wake_audio[-chunk_size:])
                        
                        if current_activity:
                            is_speaking = True
                            silent_frames = 0
                        else:
                            if is_speaking:
                                silent_frames += 1
                        
                        
                        if not asr_processing and len(post_wake_audio) >= min_asr_samples:
                            
                            if sv and not hit:
                                logger.debug("ASRæš‚åœï¼šå£°çº¹éªŒè¯æœªé€šè¿‡")
                                continue
                            
                            
                            
                            
                            
                            execute_asr = False
                            
                            if len(post_wake_audio) >= max_asr_samples:
                                logger.info(f"å”¤é†’è¯åéŸ³é¢‘è¾¾åˆ°æœ€å¤§é•¿åº¦({max_asr_samples} samples)ï¼Œæ‰§è¡ŒASR")
                                execute_asr = True
                            elif is_speaking and silent_frames >= min_silent_frames:
                                logger.info(f"å”¤é†’è¯åæ£€æµ‹åˆ°è¯­éŸ³ç»“æŸï¼Œæ‰§è¡ŒASRï¼ŒéŸ³é¢‘é•¿åº¦: {len(post_wake_audio)} samples")
                                execute_asr = True
                            elif len(post_wake_audio) > int(config.sample_rate * 5):
                                logger.info(f"å”¤é†’è¯åæŒç»­æœ‰è¯­éŸ³è¾“å…¥è¶…è¿‡5ç§’ï¼Œæ‰§è¡ŒASRï¼ŒéŸ³é¢‘é•¿åº¦: {len(post_wake_audio)} samples")
                                execute_asr = True
                            
                            
                            if execute_asr and (time.time() - last_asr_time < min_asr_interval):
                                logger.debug(f"ASRé—´éš”è¿‡çŸ­({time.time() - last_asr_time:.2f}ç§’ < {min_asr_interval}ç§’)ï¼Œè·³è¿‡")
                                execute_asr = False
                            
                            
                            if execute_asr:
                                asr_processing = True
                                last_asr_time = time.time()
                                
                                
                                #debug_asr_audio_path = f"debug_asr_audio_{int(time.time())}.wav"
                                #sf.write(debug_asr_audio_path, post_wake_audio, config.sample_rate)
                                
                                logger.debug(f"è·³è¿‡å†™å…¥ASRå¤„ç†è°ƒè¯•éŸ³é¢‘ï¼ŒéŸ³é¢‘é•¿åº¦={len(post_wake_audio)} samples")

                                
                                try:
                                    logger.info(f"å¼€å§‹å”¤é†’è¯åASRå¤„ç†ï¼ŒéŸ³é¢‘é•¿åº¦: {len(post_wake_audio)} samples")
                                    result = asr(
                                        post_wake_audio,
                                        lang.strip(),
                                        cache_asr,
                                        True
                                    )
                                    if result is not None:
                                        asr_text = result[0].get("text", "")
                                        await websocket.send_json({
                                            "code": 0,
                                            "info": json.dumps(result[0], ensure_ascii=False),
                                            "data": format_str_v3(asr_text)
                                        })
                                        logger.info(f"ASRå¤„ç†æˆåŠŸï¼Œç»“æœ: {asr_text}")
                                        
                                        
                                        for command in exit_commands:
                                            if command in asr_text:
                                                logger.info(f"æ£€æµ‹åˆ°é€€å‡ºæŒ‡ä»¤: {command}ï¼Œé€€å‡ºå”¤é†’çŠ¶æ€")
                                                flag_wakeword = False
                                                post_wake_audio = np.array([], dtype=np.float32)
                                                sv_buffer = np.array([], dtype=np.float32)
                                                is_speaking = False
                                                silent_frames = 0
                                                await websocket.send_json({
                                                    "code": 4,
                                                    "info": "å·²é€€å‡ºå”¤é†’çŠ¶æ€",
                                                    "data": {}
                                                })
                                                break
                                        
                                        
                                        last_activity_time = time.time()
                                    else:
                                        logger.warning("ASRå¤„ç†è¿”å›None")
                                except Exception as e:
                                    logger.error(f"å”¤é†’è¯åASRå¤„ç†å¼‚å¸¸: {str(e)}")
                                finally:
                                    
                                    post_wake_audio = np.array([], dtype=np.float32)
                                    asr_processing = False
                                    is_speaking = False
                                    silent_frames = 0
                    else:
                        
                        is_speaking = True
                        silent_frames = 0
                else:
                    
                    wakeword_buffer = np.append(wakeword_buffer, new_audio)
                    
                    
                    if wakeword_enabled and len(wakeword_buffer) >= min_wake_samples:
                        
                        logger.debug(f"æ‰§è¡Œå”¤é†’è¯æ£€æµ‹ï¼Œç¼“å†²åŒºé•¿åº¦: {len(wakeword_buffer)} samples")
                        
                        
                        if len(wakeword_buffer) > 0:
                            #debug_wake_audio_path = f"debug_wake_audio_{int(time.time())}.wav"
                            #sf.write(debug_wake_audio_path, wakeword_buffer, config.sample_rate)
                            
                            logger.debug(f"è·³è¿‡å†™å…¥å”¤é†’è¯æ£€æµ‹éŸ³é¢‘æ–‡ä»¶ï¼ŒéŸ³é¢‘é•¿åº¦={len(wakeword_buffer)} samples")
                        
                        
                        detected, detected_wakeword, asr_text = await process_wakeword_detection(
                            websocket, 
                            wakeword_buffer, 
                            cache_wake, 
                            wake_words
                        )
                        
                        logger.debug(f"å”¤é†’è¯æ£€æµ‹ç»“æœ: detected={detected}, wakeword={detected_wakeword}, asr_text={asr_text}")
                        
                        if detected:
                            
                            flag_wakeword = True
                            wakeword_detected_time = time.time()
                            last_activity_time = time.time()  
                            logger.success(f"å”¤é†’è¯æ£€æµ‹æˆåŠŸ: {detected_wakeword}")
                            await websocket.send_json({
                                "code": 1,
                                "info": "å”¤é†’æˆåŠŸ",
                                "data": detected_wakeword
                            })
                            
                            
                            wakeword_buffer = np.array([], dtype=np.float32)
                            
                            
                            audio_vad = np.array([], dtype=np.float32)
                            last_vad_beg = last_vad_end = -1
                            
                            
                            post_wake_audio = np.array([], dtype=np.float32)
                            sv_buffer = np.array([], dtype=np.float32)
                            asr_processing = False
                            is_speaking = False
                            silent_frames = 0
                        
                        
                        if len(wakeword_buffer) > sliding_window_samples:
                            wakeword_buffer = wakeword_buffer[-sliding_window_samples:]

            
            while len(audio_buffer_processed) >= chunk_size:
                chunk = audio_buffer_processed[:chunk_size]
                audio_buffer_processed = audio_buffer_processed[chunk_size:]
                audio_vad = np.append(audio_vad, chunk)
                
                
                if not flag_wakeword:
                    res = model_vad.generate(
                        input=chunk, 
                        cache=cache, 
                        is_final=False, 
                        chunk_size=config.chunk_size_ms
                    )
                    
                    
                    if len(res[0]["value"]):
                        for segment in res[0]["value"]:
                            
                            if segment[0] > -1:
                                last_vad_beg = segment[0]                           
                            if segment[1] > -1:
                                last_vad_end = segment[1]
                                
                                
                                if last_vad_beg > -1 and last_vad_end > -1:
                                    beg = int((last_vad_beg - offset) * config.sample_rate / 1000)
                                    end = int((last_vad_end - offset) * config.sample_rate / 1000)
                                    
                                    
                                    if beg < 0 or end > len(audio_vad) or beg >= end:
                                        logger.warning(f"æ— æ•ˆéŸ³é¢‘æ®µ: beg={beg}, end={end}")
                                        audio_vad = np.array([], dtype=np.float32)
                                        offset = last_vad_end
                                        last_vad_beg = last_vad_end = -1
                                        continue
                                    
                                    
                                    if (end - beg) < min_asr_samples:
                                        logger.debug(f"è¯­éŸ³æ®µè¿‡çŸ­({end-beg} samples < {min_asr_samples})ï¼Œè·³è¿‡ASRå¤„ç†")
                                        audio_vad = audio_vad[end:]
                                        offset = last_vad_end
                                        last_vad_beg = last_vad_end = -1
                                        continue
                                    
                                    
                                    execute_asr = not wakeword_enabled
                                    
                                    
                                    if sv and not hit and flag_wakeword:
                                        execute_asr = False
                                        logger.debug("ASRè·³è¿‡ï¼šå£°çº¹éªŒè¯æœªé€šè¿‡")
                                    
                                    
                                    if execute_asr:
                                        try:
                                            logger.info(f"å¼€å§‹ASRå¤„ç†ï¼ŒéŸ³é¢‘é•¿åº¦: {end-beg} samples")
                                            result = asr(
                                                audio_vad[beg:end],
                                                lang.strip(),
                                                cache_asr,
                                                True
                                            )
                                            if result is not None:
                                                await websocket.send_json({
                                                    "code": 0,
                                                    "info": json.dumps(result[0], ensure_ascii=False),
                                                    "data": format_str_v3(result[0]['text'])
                                                })
                                        except Exception as e:
                                            logger.error(f"ASRå¤„ç†å¼‚å¸¸: {str(e)}")
                                    else:
                                        logger.debug(f"ASRæœªæ‰§è¡Œï¼Œexecute_asr={execute_asr}")
                                    
                                    
                                    audio_vad = audio_vad[end:]
                                    offset = last_vad_end
                                    last_vad_beg = last_vad_end = -1
                                    hit = False

    except WebSocketDisconnect:
        logger.info("å®¢æˆ·ç«¯ä¸»åŠ¨æ–­å¼€è¿æ¥")
    except Exception as e:
        logger.critical(f"WebSocketä¸¥é‡é”™è¯¯: {str(e)}\n{traceback.format_exc()}")
        try:
            await websocket.close(code=1011, reason="Server Error")
        except:
            pass
    finally:
        
        audio_buffer_raw = b""
        audio_buffer_processed = np.array([], dtype=np.float32)
        audio_vad = np.array([], dtype=np.float32)
        wakeword_buffer = np.array([], dtype=np.float32)
        post_wake_audio = np.array([], dtype=np.float32)
        sv_buffer = np.array([], dtype=np.float32)
        cache.clear()
        cache_asr.clear()
        cache_wake.clear()
        logger.info("è¿æ¥èµ„æºå·²é‡Šæ”¾")

def check_vad_activity(audio_data):
    """ä½¿ç”¨VADæ£€æµ‹éŸ³é¢‘ä¸­æ˜¯å¦æœ‰è¯­éŸ³æ´»åŠ¨"""
    if len(audio_data) < 3200:  
        return False
        
    
    audio_int16 = (audio_data * 32767).astype(np.int16).tobytes()
    
    
    vad = webrtcvad.Vad(3)  
    chunk_size = 320  # 20ms @16kHz
    active_chunks = 0
    
    for i in range(0, len(audio_int16), chunk_size):
        chunk = audio_int16[i:i+chunk_size]
        if len(chunk) == chunk_size:
            if vad.is_speech(chunk, 16000):
                active_chunks += 1
    
    
    return active_chunks > (len(audio_int16) // chunk_size) * 0.5



async def process_wakeword_detection(websocket, audio_buffer, cache, wake_words):
    """ç‹¬ç«‹çš„å”¤é†’è¯æ£€æµ‹å‡½æ•°ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æœºåˆ¶å’Œæ‹¼éŸ³åŒ¹é…"""
    try:
        
        if len(audio_buffer) == 0:
            logger.warning("å”¤é†’è¯æ£€æµ‹ï¼šéŸ³é¢‘ç¼“å†²åŒºä¸ºç©º")
            return False, "", ""
            
        
        logger.debug(f"å”¤é†’è¯æ£€æµ‹è¾“å…¥éŸ³é¢‘é•¿åº¦: {len(audio_buffer)} samples ({len(audio_buffer)/config.sample_rate:.2f} seconds)")
        
        result = model_asr.generate(
            input=audio_buffer,
            cache=cache,
            language="zh",
            use_itn=True,
            batch_size_s=1
        )
        
        
        detected = False
        detected_wakeword = ""
        asr_text = ""
        
        if result and len(result) > 0:
            asr_text = result[0].get("text", "").lower()
            logger.debug(f"å”¤é†’è¯ASRè¯†åˆ«ç»“æœ: {asr_text}")
            
            
            clean_text = re.sub(r'<[^>]+>', '', asr_text)
            
            clean_text = re.sub(r'[^\w\s]', '', clean_text)
            logger.debug(f"å¤„ç†åçš„æ–‡æœ¬ç”¨äºåŒ¹é…: '{clean_text}'")
            
            
            asr_pinyin = text_to_pinyin(clean_text)
            logger.debug(f"ASRæ–‡æœ¬çš„æ‹¼éŸ³: '{asr_pinyin}'")
            
            
            for wake_word in wake_words:
                
                clean_wakeword = re.sub(r'[^\w\s]', '', wake_word)
                
                
                wake_pinyin = text_to_pinyin(clean_wakeword)
                logger.debug(f"å”¤é†’è¯ '{wake_word}' çš„æ‹¼éŸ³: '{wake_pinyin}'")
                
                
                if wake_pinyin in asr_pinyin:
                    detected = True
                    detected_wakeword = wake_word
                    logger.debug(f"æ‹¼éŸ³åŒ¹é…æˆåŠŸ: '{wake_pinyin}' in '{asr_pinyin}'")
                    break
        
        return detected, detected_wakeword, asr_text
    except Exception as e:
        logger.error(f"å”¤é†’è¯æ£€æµ‹å¼‚å¸¸: {str(e)}")
        return False, "", ""

def text_to_pinyin(text):
    """å°†ä¸­æ–‡æ–‡æœ¬è½¬æ¢ä¸ºæ‹¼éŸ³ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†éš”æ¯ä¸ªå­—çš„æ‹¼éŸ³"""
    if not text:
        return ""
        
    
    pinyin_list = pypinyin.pinyin(text, style=pypinyin.NORMAL)
    
    
    return ' '.join([item[0] for item in pinyin_list])

@app.post("/api/uploadSpeaker")
async def upload_speaker(file: UploadFile = File(...)):
    try:
        upload_dir = os.path.join(os.getcwd(), "speaker")
        os.makedirs(upload_dir, exist_ok=True)

        timestamp = int(time.time() * 1000)
        filename = f"speaker_{timestamp}.wav"
        file_path = os.path.join(upload_dir, filename)

        content = await file.read()

        audio = AudioSegment.from_file(io.BytesIO(content))
        audio = audio.set_frame_rate(config.sample_rate).set_channels(1).set_sample_width(2)
        audio.export(file_path, format="wav")

        data, sr = sf.read(file_path, dtype="float32")

        global reg_spks_files, reg_spks
        reg_spks_files = [file_path]  
        reg_spks = reg_spk_init(reg_spks_files)
        
        
        speaker_id = os.path.splitext(filename)[0]
        registered_speakers[speaker_id] = True
        save_registered_speakers(registered_speakers)

        logger.info(f"Uploaded speaker wav saved as {file_path}")
        logger.info(f"å½“å‰æ³¨å†Œç”¨æˆ·åˆ—è¡¨: {list(registered_speakers.keys())}")
        return {"code": 0, "info": "ä¸Šä¼ æˆåŠŸ", "data": filename}
    except Exception as e:
        logger.error(f"ä¸Šä¼ å£°çº¹éŸ³é¢‘å¤±è´¥: {e}")
        return JSONResponse(status_code=500, content={"code": 500, "info": f"ä¸Šä¼ å¤±è´¥: {e}", "data": ""})


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the FastAPI app with a specified port.")
    parser.add_argument('--port', type=int, default=6007, help='Port number to run the FastAPI app on.')
    args = parser.parse_args()
    uvicorn.run(app, host="0.0.0.0", port=args.port)